{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/ASMSA/trpcage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = 2\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS']=str(threads)\n",
    "import tensorflow as tf\n",
    "\n",
    "# PyTorch favours OMP_NUM_THREADS in environment\n",
    "import torch\n",
    "\n",
    "# Tensorflow needs explicit cofig calls\n",
    "tf.config.threading.set_inter_op_parallelism_threads(threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras as k\n",
    "import numpy as np\n",
    "import mdtraj as md\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import datetime\n",
    "from asmsa_callbacks import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open('inputs.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_number_of_neurons(layers, seed):\n",
    "    neurons = [seed]\n",
    "\n",
    "    tmp = seed\n",
    "    for _ in range(layers-1):\n",
    "        tmp = int(tmp / 2)\n",
    "        neurons.append(tmp)\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAEClassModel(k.models.Model):\n",
    "    def __init__(self,\n",
    "            n_features,\n",
    "            n_classes,\n",
    "            enc_layers, enc_seed,\n",
    "            disc_layers, disc_seed,\n",
    "            af='gelu',\n",
    "            bn_momentum=0.8, leak_alpha=0.2,\n",
    "            latent_dim=2,\n",
    "            dist_threshold=5.\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "# https://doi.org/10.48550/arXiv.1511.05644, Sect. 7 / Fig. 10\n",
    "        self.n_classes = n_classes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dist_threshold2 = dist_threshold*dist_threshold\n",
    "        \n",
    "        enc_neurons = _compute_number_of_neurons(enc_layers, enc_seed) \n",
    "        disc_neurons = _compute_number_of_neurons(disc_layers, disc_seed)\n",
    "\n",
    "        inp = k.Input(shape=(n_features,),name='inp')\n",
    "        out = inp\n",
    "\n",
    "        for n in range(enc_layers):\n",
    "            out = k.layers.Dense(enc_neurons[n],activation=af,name=f'enc_{n}')(out)\n",
    "            out = k.layers.BatchNormalization(momentum=bn_momentum, name=f'enc_bn_{n}')(out)\n",
    "\n",
    "        z = k.layers.Dense(latent_dim,name='latent_z')(out)\n",
    "        y = k.layers.Dense(n_classes,activation='softmax',name='latent_y')(out)\n",
    "\n",
    "        chs = k.layers.Dense(latent_dim,name='cluster_heads')\n",
    "        hz = chs(y)\n",
    "        d = (math.pow(n_classes,1./latent_dim) - 1) * dist_threshold / 2 / 3 # XXX\n",
    "        chs.set_weights([\n",
    "            np.random.uniform(low=-d,high=d,size=(n_classes,latent_dim)),\n",
    "            np.random.normal(size=(latent_dim,))\n",
    "        ])\n",
    "        \n",
    "        latent = k.layers.Add(name='add_z_hz')([z,hz])\n",
    "\n",
    "        out = latent\n",
    "        for n in reversed(range(enc_layers)):\n",
    "            out = k.layers.Dense(enc_neurons[n],activation=af,name=f'dec_{n}')(out)\n",
    "            out = k.layers.BatchNormalization(momentum=bn_momentum, name=f'dec_bn_{n}')(out)\n",
    "\n",
    "        dec_out = k.layers.Dense(n_features,name='dec_out')(out)\n",
    "\n",
    "        out = y\n",
    "        for n in range(disc_layers):\n",
    "            out = k.layers.Dense(disc_neurons[n],name=f'y_disc_{n}')(out)\n",
    "            out = k.layers.LeakyReLU(negative_slope=leak_alpha,name=f'y_disc_relu_{n}')(out)\n",
    "        \n",
    "        y_disc_out = k.layers.Dense(1,name='y_disc_out')(out)\n",
    "\n",
    "        out = z\n",
    "        for n in range(disc_layers):\n",
    "            out = k.layers.Dense(disc_neurons[n],name=f'z_disc_{n}')(out)\n",
    "            out = k.layers.LeakyReLU(negative_slope=leak_alpha,name=f'z_disc_relu_{n}')(out)\n",
    "\n",
    "        z_disc_out = k.layers.Dense(1,name='z_disc_out')(out)\n",
    "\n",
    "        self.enc = k.Model(inputs=inp,outputs=[y,z])\n",
    "        self.sum = k.Model(inputs=[y,z],outputs=latent)\n",
    "        self.dec = k.Model(inputs=latent,outputs=dec_out)\n",
    "        self.heads = k.Model(inputs=y,outputs=hz)\n",
    "        # self.ae = k.Model(inputs=inp,outputs=dec_out)\n",
    "        self.y_disc = k.Model(inputs=y,outputs=y_disc_out)\n",
    "        self.z_disc = k.Model(inputs=z,outputs=z_disc_out)\n",
    "\n",
    "    def compile(self,optimizer=None,lr=None):\n",
    "        \n",
    "        if optimizer is None:\n",
    "            optimizer = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=1e-4,\n",
    "            weight_decay=1e-5,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999\n",
    "        )\n",
    "            opt = optimizer\n",
    "            \n",
    "        else:\n",
    "            opt = optimizer\n",
    "\n",
    "        self.ae_loss = k.losses.Huber() #MeanSquaredError()\n",
    "\n",
    "        super().compile(optimizer=opt,loss=self.ae_loss)\n",
    "        self.optimizer.build(self.enc.trainable_weights+self.sum.trainable_weights+self.dec.trainable_weights+self.y_disc.trainable_weights+self.z_disc.trainable_weights)\n",
    "\n",
    "        self.enc.compile()\n",
    "        self.dec.compile()\n",
    "        self.y_disc.compile()\n",
    "        self.z_disc.compile()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self,in_batch):\n",
    "        if isinstance(in_batch, tuple):\n",
    "            batch = in_batch[0]\n",
    "        else:\n",
    "            batch = in_batch\n",
    "\n",
    "        # autoencoder\n",
    "        with tf.GradientTape() as aet:\n",
    "            y,z = self.enc(batch)\n",
    "            rec = self.dec(self.sum([y,z]))\n",
    "            ael = self.ae_loss(batch,rec)\n",
    "    \n",
    "        aew = self.enc.trainable_weights + self.sum.trainable_weights + self.dec.trainable_weights\n",
    "        aeg = aet.gradient(ael,aew)\n",
    "        self.optimizer.apply_gradients(zip(aeg,aew))\n",
    "    \n",
    "        # categoric discriminator\n",
    "        idx = tf.random.uniform((batch.shape[0],), minval=0, maxval=self.n_classes, dtype=tf.int32)\n",
    "        randy = tf.one_hot(idx, depth=self.n_classes)\n",
    "    \n",
    "        # binary crossentropy from logits\n",
    "        with tf.GradientTape() as yt:\n",
    "            nyp = self.y_disc(y)\n",
    "            nyp *= tf.random.uniform(tf.shape(nyp), 1., 1.05)\n",
    "            nyl = tf.reduce_mean(nyp,axis=0)\n",
    "    \n",
    "            pyp = self.y_disc(randy)\n",
    "            pyp *= tf.random.uniform(tf.shape(pyp), 1., 1.05)\n",
    "            pyl = -tf.reduce_mean(pyp,axis=0)\n",
    "            y_disc_loss = (nyl + pyl) * 1e-5 #XXX\n",
    "    \n",
    "        yg = yt.gradient(y_disc_loss,self.y_disc.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(yg,self.y_disc.trainable_weights))\n",
    "    \n",
    "        # cheet it\n",
    "        with tf.GradientTape() as yct:\n",
    "            yc = self.y_disc(self.enc(batch)[0])\n",
    "            yc *= tf.random.uniform(tf.shape(yc), 1., 1.05)\n",
    "            ycl = -tf.reduce_mean(yc, axis=0) * 1e-5 #XXX\n",
    "    \n",
    "        ycg = yct.gradient(ycl,self.enc.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(ycg,self.enc.trainable_weights))\n",
    "            \n",
    "        # intra category discriminator\n",
    "        randz = tf.random.normal(shape=(batch.shape[0], self.latent_dim))\n",
    "    \n",
    "        with tf.GradientTape() as zt:\n",
    "            nzp = self.z_disc(z)\n",
    "            nzp *= tf.random.uniform(tf.shape(nzp), 1., 1.05)\n",
    "            nzl = tf.reduce_mean(nzp,axis=0)\n",
    "    \n",
    "            pzp = self.z_disc(randz)\n",
    "            pzp *= tf.random.uniform(tf.shape(pzp), 1., 1.05)\n",
    "            pzl = -tf.reduce_mean(pzp,axis=0)\n",
    "            z_disc_loss = nzl + pzl\n",
    "    \n",
    "        zg = zt.gradient(z_disc_loss,self.z_disc.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(zg,self.z_disc.trainable_weights))\n",
    "            \n",
    "        # cheet it\n",
    "        with tf.GradientTape() as zct:\n",
    "            zc = self.z_disc(self.enc(batch)[1])\n",
    "            zc *= tf.random.uniform(tf.shape(zc), 1., 1.05)\n",
    "            zcl = -tf.reduce_mean(zc, axis=0)\n",
    "    \n",
    "        zcg = zct.gradient(zcl,self.enc.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(zcg,self.enc.trainable_weights))\n",
    "        \n",
    "        # keep cluster heads apart\n",
    "        randc = tf.linalg.diag(tf.random.uniform((self.n_classes,),0.95,1.05))\n",
    "        with tf.GradientTape() as ht:\n",
    "            ch = self.heads(randc)\n",
    "            norms = tf.reduce_sum(tf.square(ch), axis=1, keepdims=True)  # shape=(N,1)\n",
    "            dists_squared = norms - 2 * tf.matmul(ch, ch, transpose_b=True) + tf.transpose(norms)\n",
    "            small_dists = tf.boolean_mask(tf.exp(-dists_squared), dists_squared < self.dist_threshold2)\n",
    "            hl = tf.reduce_sum(small_dists)/self.dist_threshold2/self.n_classes/self.n_classes * 21. #XXX\n",
    "\n",
    "        hg = ht.gradient(hl,self.heads.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(hg,self.heads.trainable_weights))\n",
    "\n",
    "        return {'ae_loss' : ael}\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, in_batch):\n",
    "        \"\"\"Test step per il monitoraggio della validazione durante il training\"\"\"\n",
    "        if isinstance(in_batch, tuple):\n",
    "            batch = in_batch[0]\n",
    "        else:\n",
    "            batch = in_batch\n",
    "\n",
    "        y, z = self.enc(batch, training=False)\n",
    "        rec = self.dec(self.sum([y, z]), training=False)\n",
    "        val_ae_loss = self.ae_loss(batch, rec)\n",
    "        \n",
    "        idx = tf.random.uniform((batch.shape[0],), minval=0, maxval=self.n_classes, dtype=tf.int32)\n",
    "        randy = tf.one_hot(idx, depth=self.n_classes)\n",
    "        \n",
    "        nyp = self.y_disc(y, training=False)\n",
    "        nyl = tf.reduce_mean(nyp, axis=0)\n",
    "        pyp = self.y_disc(randy, training=False)\n",
    "        pyl = -tf.reduce_mean(pyp, axis=0)\n",
    "        val_y_disc_loss = (nyl + pyl) * 1e-5\n",
    "        \n",
    "        yc = self.y_disc(y, training=False)\n",
    "        val_y_cheat_loss = -tf.reduce_mean(yc, axis=0) * 1e-5\n",
    "        \n",
    "        randz = tf.random.normal(shape=(batch.shape[0], self.latent_dim))\n",
    "        nzp = self.z_disc(z, training=False)\n",
    "        nzl = tf.reduce_mean(nzp, axis=0)\n",
    "        pzp = self.z_disc(randz, training=False)\n",
    "        pzl = -tf.reduce_mean(pzp, axis=0)\n",
    "        val_z_disc_loss = nzl + pzl\n",
    "        \n",
    "        # Z cheat validation loss\n",
    "        zc = self.z_disc(z, training=False)\n",
    "        val_z_cheat_loss = -tf.reduce_mean(zc, axis=0)\n",
    "        \n",
    "        # Cluster head validation loss\n",
    "        randc = tf.linalg.diag(tf.ones((self.n_classes,)))  # No random noise for validation\n",
    "        ch = self.heads(randc, training=False)\n",
    "        norms = tf.reduce_sum(tf.square(ch), axis=1, keepdims=True)\n",
    "        dists_squared = norms - 2 * tf.matmul(ch, ch, transpose_b=True) + tf.transpose(norms)\n",
    "        small_dists = tf.boolean_mask(tf.exp(-dists_squared), dists_squared < self.dist_threshold2)\n",
    "        val_cluster_head_loss = tf.reduce_sum(small_dists)/self.dist_threshold2/self.n_classes/self.n_classes * 21.\n",
    "\n",
    "        return {'ae_loss': val_ae_loss,}\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,inp):\n",
    "        return self.sum(self.enc(inp))\n",
    "\n",
    "    def call_enc(self,inp):\n",
    "        return self.call(inp)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX: essential hps manually\n",
    "batch_size = 64\n",
    "best_enc_seed=128\n",
    "best_disc_seed=128 \n",
    "ae_layers=2 \n",
    "disc_layers=3\n",
    "#learning_rate=0.00002\n",
    "n_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.data.Dataset.load('datasets/intcoords/train')\n",
    "\n",
    "# get batched version of dataset to feed to AAE model for training\n",
    "X_train_batched = X_train.batch(batch_size,drop_remainder=True)\n",
    "\n",
    "# get numpy version for visualization purposes\n",
    "X_train_np = np.stack(list(X_train))\n",
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tf.data.Dataset.load('datasets/intcoords/test')\n",
    "\n",
    "# get numpy version for visualization purposes\n",
    "X_test_np = np.stack(list(X_test))\n",
    "X_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a632a929-bdbf-4071-9f51-622fd10956af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = tf.data.Dataset.load('datasets/intcoords/validate').batch(batch_size,drop_remainder=True)\n",
    "X_val_np = np.stack(list(X_val))\n",
    "X_val_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = AAEClassModel(n_features=X_train_np.shape[1],n_classes=n_classes,enc_layers=ae_layers,enc_seed=best_enc_seed,disc_layers=disc_layers,disc_seed=best_disc_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e7478-dde7-4efe-afb8-a2cd73b917db",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/class_traom/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "cb = callbacks(log_dir, m, X_test_np,freq=20, monitor=\"val_ae_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.compile()#lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb3617-9752-4342-94df-975d90ff7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(X_train_batched,epochs=500,verbose=2, validation_data=X_val,\n",
    "     callbacks = cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asmsa.visualizer as visualizer\n",
    "\n",
    "visualizer.Visualizer(figsize=(12,3)).make_visualization(m(X_train_np[::10,:]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.Visualizer(figsize=(12,3)).make_visualization(m(X_test_np).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = md.load('../DE-Shaw/trpcage_red.xtc',top=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31edc610-4f21-4f7b-a219-0b398e92da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all dataset\n",
    "X_all = tf.data.Dataset.load('datasets/intcoords/X_all')\n",
    "\n",
    "# get batched version of dataset to feed to AAE model for prediction\n",
    "X_all_batched = X_all.batch(batch_size,drop_remainder=True)\n",
    "\n",
    "# get numpy version for testing purposes\n",
    "X_all_np = np.stack(list(X_all))\n",
    "X_all_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e148490-bba6-4b5d-806a-70cdf9fcf056",
   "metadata": {},
   "outputs": [],
   "source": [
    "tica = np.load(\"../ASMSA_DE/tica3.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lows = m(X_all_np).numpy()\n",
    "rg = md.compute_rg(tr)\n",
    "base = md.load(conf)\n",
    "rmsd = md.rmsd(tr,base[0])\n",
    "cmap = plt.get_cmap('rainbow')\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=rg,cmap=cmap,s=1) #tica[:,0]\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.title(\"Rg\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=rmsd,cmap=cmap,s=1) #tica[:,1]\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.title(\"RMSD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5374c9-2574-4d7f-989c-6e44cae940e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lows = m(X_all_np).numpy()\n",
    "rg = md.compute_rg(tr)\n",
    "base = md.load(conf)\n",
    "rmsd = md.rmsd(tr,base[0])\n",
    "cmap = plt.get_cmap('Dark2')\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=rg,cmap=cmap,s=1) #tica[:,0]\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.title(\"Rg\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=rmsd,cmap=cmap,s=1) #tica[:,1]\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.title(\"RMSD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7412b-7c7e-4e79-8fdc-a980dc380673",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = md.load_xtc(\"../DE-Shaw/trpcage_red.xtc\", top=conf)\n",
    "\n",
    "dssp = md.compute_dssp(traj, simplified=True)  # simplified=True → 'H' (alpha helix), 'E' (beta sheet), etc.\n",
    "\n",
    "alpha_content_per_frame = np.mean(dssp == 'H', axis=1)\n",
    "\n",
    "average_alpha_helix_content = np.mean(alpha_content_per_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a424ac-f488-4c1f-80d4-8ff2c17a97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lows = m(X_all_np).numpy()\n",
    "rg = md.compute_rg(tr)\n",
    "base = md.load(conf)\n",
    "rmsd = md.rmsd(tr,base[0])\n",
    "cmap = plt.get_cmap('rainbow')\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=alpha_content_per_frame,cmap=cmap,s=1) #tica[:,0]\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.title(\"alpha_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926c418-1e72-4e6e-bb7b-13463fa22737",
   "metadata": {},
   "outputs": [],
   "source": [
    "lows = m(X_all_np).numpy()\n",
    "rg = md.compute_rg(tr)\n",
    "base = md.load(conf)\n",
    "rmsd = md.rmsd(tr,base[0])\n",
    "cmap = plt.get_cmap('rainbow')\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=tica[:,0],cmap=cmap,s=1) #\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.title(\"tica1\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(lows[:,0],lows[:,1],marker='.',c=tica[:,1],cmap=cmap,s=1) #\n",
    "plt.colorbar(cmap=cmap)\n",
    "plt.title(\"tica2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = m.heads(tf.linalg.diag(tf.constant([1.] * n_classes))).numpy()\n",
    "c = np.log(np.sum(m.enc(X_all_np)[0].numpy(),axis=0))\n",
    "plt.scatter(heads[:,0],heads[:,1],c=c,cmap='Dark2')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = m.enc(X_all_np)[1].numpy()\n",
    "plt.scatter(z[:,0],z[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
