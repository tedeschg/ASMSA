{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as k\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_number_of_neurons(layers, seed):\n",
    "    neurons = [seed]\n",
    "\n",
    "    tmp = seed\n",
    "    for _ in range(layers-1):\n",
    "        tmp = int(tmp / 2)\n",
    "        neurons.append(tmp)\n",
    "    return neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_compute_number_of_neurons(3,96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAEClassModel(k.models.Model):\n",
    "    def __init__(self,\n",
    "            n_features,\n",
    "            n_classes,\n",
    "            enc_layers, enc_seed,\n",
    "            disc_layers, disc_seed,\n",
    "            af='gelu',\n",
    "            bn_momentum=0.8, leak_alpha=0.2,\n",
    "            latent_dim=2,\n",
    "            dist_threshold=2.\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "# https://doi.org/10.48550/arXiv.1511.05644, Sect. 7 / Fig. 10\n",
    "        self.n_classes = n_classes\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dist_threshold2 = dist_threshold*dist_threshold\n",
    "        \n",
    "        enc_neurons = _compute_number_of_neurons(enc_layers, enc_seed) \n",
    "        disc_neurons = _compute_number_of_neurons(disc_layers, disc_seed)\n",
    "\n",
    "        inp = k.Input(shape=(n_features,),name='inp')\n",
    "        out = inp\n",
    "\n",
    "        for n in range(enc_layers):\n",
    "            out = k.layers.Dense(enc_neurons[n],activation=af,name=f'enc_{n}')(out)\n",
    "            out = k.layers.BatchNormalization(momentum=bn_momentum, name=f'enc_bn_{n}')(out)\n",
    "\n",
    "        z = k.layers.Dense(latent_dim,name='latent_z')(out)\n",
    "        y = k.layers.Dense(n_classes,activation='softmax',name='latent_y')(out)\n",
    "\n",
    "        hz = k.layers.Dense(latent_dim,name='cluster_heads')(y)\n",
    "        latent = k.layers.Add(name='add_z_hz')([z,hz])\n",
    "\n",
    "        out = latent\n",
    "        for n in reversed(range(enc_layers)):\n",
    "            out = k.layers.Dense(enc_neurons[n],activation=af,name=f'dec_{n}')(out)\n",
    "            out = k.layers.BatchNormalization(momentum=bn_momentum, name=f'dec_bn_{n}')(out)\n",
    "\n",
    "        dec_out = k.layers.Dense(n_features,name='dec_out')(out)\n",
    "\n",
    "        out = y\n",
    "        for n in range(disc_layers):\n",
    "            out = k.layers.Dense(disc_neurons[n],name=f'y_disc_{n}')(out)\n",
    "            out = k.layers.LeakyReLU(negative_slope=leak_alpha,name=f'y_disc_relu_{n}')(out)\n",
    "        \n",
    "        y_disc_out = k.layers.Dense(1,name='y_disc_out')(out)\n",
    "\n",
    "        out = z\n",
    "        for n in range(disc_layers):\n",
    "            out = k.layers.Dense(disc_neurons[n],name=f'z_disc_{n}')(out)\n",
    "            out = k.layers.LeakyReLU(negative_slope=leak_alpha,name=f'z_disc_relu_{n}')(out)\n",
    "\n",
    "        z_disc_out = k.layers.Dense(1,name='z_disc_out')(out)\n",
    "\n",
    "        self.enc = k.Model(inputs=inp,outputs=[y,z])\n",
    "        self.sum = k.Model(inputs=[y,z],outputs=latent)\n",
    "        self.dec = k.Model(inputs=latent,outputs=dec_out)\n",
    "        self.heads = k.Model(inputs=y,outputs=hz)\n",
    "        # self.ae = k.Model(inputs=inp,outputs=dec_out)\n",
    "        self.y_disc = k.Model(inputs=y,outputs=y_disc_out)\n",
    "        self.z_disc = k.Model(inputs=z,outputs=z_disc_out)\n",
    "\n",
    "    def compile(self,optimizer,lr=None):\n",
    "        if optimizer is None:\n",
    "            self.opt = k.optimizers.Adam(learning_rate=lr)\n",
    "        else:\n",
    "            self.opt = optimizer\n",
    "\n",
    "        self.ae_loss = k.losses.MeanSquaredError()\n",
    "\n",
    "        self.y_enc.compile()\n",
    "        self.z_enc.compile()\n",
    "        self.enc.compile()\n",
    "        self.dec.compile()\n",
    "        self.y_disc.compile()\n",
    "        self.z_disc.compile()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self,in_batch):\n",
    "        if isinstance(in_batch, tuple):\n",
    "            batch = in_batch[0]\n",
    "        else:\n",
    "            batch = in_batch\n",
    "\n",
    "        # autoencoder\n",
    "        with tf.GradientTape() as aet:\n",
    "            y,z = self.enc(batch)\n",
    "            rec = self.dec(self.sum([y,z]))\n",
    "            ael = self.ae_loss(batch,rec)\n",
    "    \n",
    "        aew = self.enc.trainable_weights + self.sum.trainable_weights + self.dec.trainable_weights\n",
    "        aeg = aet.gradient(ael,aew)\n",
    "        self.opt.apply_gradients(zip(aeg,aew))\n",
    "    \n",
    "        # categoric discriminator\n",
    "        idx = tf.random.uniform((batch.shape[0],), minval=0, maxval=self.n_classes, dtype=tf.int32)\n",
    "        randy = tf.one_hot(idx, depth=self.n_classes)\n",
    "    \n",
    "        # binary crossentropy from logits\n",
    "        with tf.GradientTape() as yt:\n",
    "            nyp = self.y_disc(y)\n",
    "            nyp *= tf.random.uniform(tf.shape(nyp), 1., 1.05)\n",
    "            nyl = tf.reduce_mean(nyp,axis=0)\n",
    "    \n",
    "            pyp = self.y_disc(randy)\n",
    "            pyp *= tf.random.uniform(tf.shape(pyp), 1., 1.05)\n",
    "            pyl = -tf.reduce_mean(pyp,axis=0)\n",
    "            y_disc_loss = nyl + pyl\n",
    "    \n",
    "        yg = yt.gradient(y_disc_loss,self.y_disc.trainable_weights)\n",
    "        self.opt.apply_gradients(zip(yg,self.disc.trainable_weights))\n",
    "    \n",
    "        # cheet it\n",
    "        with tf.GradientTape() as yct:\n",
    "            yc = self.y_disc(self.enc(batch))\n",
    "            yc *= tf.random.uniform(tf.shape(yc), 1., 1.05)\n",
    "            ycl = -tf.reduce_mean(yc, axis=0)\n",
    "    \n",
    "        ycg = yct.gradient(ycl,self.enc.trainable_weights)\n",
    "        self.opt.apply_gradient(zip(ycg,self.enc.trainable_weights))\n",
    "            \n",
    "        # intra category discriminator\n",
    "        randz = tf.random.normal(shape=(batch.shape[0], self.latent_dim))\n",
    "    \n",
    "        with tf.GradientTape() as zt:\n",
    "            nzp = self.z_disc(z)\n",
    "            nzp *= tf.random.uniform(tf.shape(nzp), 1., 1.05)\n",
    "            nzl = tf.reduce_mean(nzp,axis=0)\n",
    "    \n",
    "            pzp = self.z_disc(randz)\n",
    "            pzp *= tf.random.uniform(tf.shape(pzp), 1., 1.05)\n",
    "            pzl = -tf.reduce_mean(pzp,axis=0)\n",
    "            z_disc_loss = nzl + pzl\n",
    "    \n",
    "        zg = zt.gradient(z_disc_loss,self.z_disc.trainable_weights)\n",
    "        self.opt.apply_gradients(zip(zg,self.disc.trainable_weights))\n",
    "            \n",
    "        # cheet it\n",
    "        with tf.GradientTape() as zct:\n",
    "            zc = self.z_disc(self.enc(batch))\n",
    "            zc *= tf.random.uniform(tf.shape(zc), 1., 1.05)\n",
    "            zcl = -tf.reduce_mean(zc, axis=0)\n",
    "    \n",
    "        zcg = yct.gradient(zcl,self.enc.trainable_weights)\n",
    "        self.opt.apply_gradient(zip(zcg,self.enc.trainable_weights))\n",
    "        \n",
    "        # keep cluster heads apart\n",
    "        randc = tf.linalg.diag(tf.random.uniform((self.n_classes,),0.95,1.05))\n",
    "        with tf.GradientTape() as ht:\n",
    "            ch = self.heads(randc)\n",
    "            norms = tf.reduce_sum(tf.square(ch), axis=1, keepdims=True)  # shape=(N,1)\n",
    "            dists_squared = norms - 2 * tf.matmul(ch, ch, transpose_b=True) + tf.transpose(norms)\n",
    "            small_dists = tf.mask(dists_squared, dists < self.dist_threshold2)\n",
    "            hl = tf.sum(tf.exp(-small_dists))\n",
    "\n",
    "        hg = ht.gradient(hl,self.heads.trainable_weights)\n",
    "        self.opt.apply_gradient(zip(hg,self.heads.trainable_weights))\n",
    "\n",
    "        return { \n",
    "            'ae_loss' : ael,\n",
    "            'y_disc_loss': y_disc_loss,\n",
    "            'z_disc_loss': z_disc_loss,\n",
    "            'y_cheat_loss': ycl,\n",
    "            'z_cheat_loss': zcl,\n",
    "            'cluster_head_loss': hl\n",
    "        }\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,inp):\n",
    "        return self.sum(self.enc(inp))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = AAEClassModel(n_features=123,n_classes=14,enc_layers=3,enc_seed=96,disc_layers=3,disc_seed=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m.enc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m.dec.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m.y_disc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "m.sum.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "randc = tf.linalg.diag(tf.random.uniform((m.n_classes,),0.95,1.05))\n",
    "m.heads(randc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.uniform((m.n_classes,),0.95,1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "m(tf.random.normal((4,123)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
